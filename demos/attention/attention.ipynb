{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOsHiz63_Cjr"
      },
      "source": [
        "# Attention\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "[<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c8/Scout_Girl_in_Concentration.jpg\" width=\"200\" align=\"left\" style=\"margin:5px 10px\">](https://en.wikipedia.org/wiki/Attention) This little demo illustrates some of the concepts behind attention in transformers, using word embeddings. The goal is simply to show how the basic calculations underlying attention work, and how weighting embeddings with attention can change the words that are similar. As implemented in Transformers, attention can be notoriously difficult to understand.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjL01xrl_Cjt",
        "outputId": "4bde0752-6c4e-4987-f50e-ff858c8336db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: networkx in /Users/lafferty/opt/anaconda3/envs/iML/lib/python3.8/site-packages (3.1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install networkx\n",
        "import networkx as nx\n",
        "import gensim\n",
        "import gensim.matutils as matutils\n",
        "import gensim.downloader as gdl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iHu5lF7_Cju",
        "outputId": "c4237415-4ce2-4226-a9d3-27ac5ff4a602"
      },
      "outputs": [],
      "source": [
        "embedding = gdl.load(\"glove-wiki-gigaword-100\")\n",
        "embedding.init_sims()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y4Px5lpW_Cju"
      },
      "outputs": [],
      "source": [
        "def most_similar(context, topn=10):\n",
        "    dists = np.dot(embedding.vectors_norm, context)\n",
        "    best = matutils.argsort(dists, topn=topn, reverse=True)\n",
        "    return best\n",
        "\n",
        "def attention(word, context, normalize=True, beta=1):\n",
        "    ei =  embedding.get_vector(word, norm=True)\n",
        "    attn = np.zeros(len(context))\n",
        "    for j in np.arange(len(context)):\n",
        "        ej = embedding.get_vector(context[j], norm=True)\n",
        "        attn[j] = np.dot(ei,ej)\n",
        "\n",
        "    attn = np.exp(beta*attn)/np.sum(np.exp(beta*attn))\n",
        "    attn = np.round(np.round(attn, 3) / np.sum(np.round(attn, 3)), 3)\n",
        "\n",
        "    return attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "elHzw0MW_Cju"
      },
      "outputs": [],
      "source": [
        "def display_attention_weights(keys, query):\n",
        "    sentence = keys\n",
        "    output = query\n",
        "\n",
        "    sentence = sentence.split()\n",
        "    keys = ['%s ' % word for word in sentence]\n",
        "    query = [' %s' % output]\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(keys, bipartite=0)\n",
        "    G.add_nodes_from(query, bipartite=1)\n",
        "\n",
        "    attn = attention(word=output, context=sentence, beta=5)\n",
        "\n",
        "    for j in range(len(attn)):\n",
        "        weight = np.round(attn[j], 3)\n",
        "        G.add_edge('%s ' % sentence[j], ' %s' % output, weight=weight)\n",
        "\n",
        "    pos = nx.bipartite_layout(G, keys)\n",
        "\n",
        "    for j in range(len(keys)):\n",
        "        pos[keys[j]][1] = -j\n",
        "\n",
        "    for j in range(len(query)):\n",
        "        pos[query[j]][1] = -j-len(keys)/2\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(6, 5)\n",
        "    edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
        "    edges = G.edges()\n",
        "    weights = [10*G[u][v]['weight'] for u,v in edges]\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
        "    nx.draw(G, pos=pos, alpha=.75, width=weights)\n",
        "\n",
        "\n",
        "    for word in keys:\n",
        "        x, y = pos[word]\n",
        "        plt.text(x, y, s=\"%s  \" % word, horizontalalignment='right')\n",
        "\n",
        "    for word in query:\n",
        "        x, y = pos[word]\n",
        "        plt.text(x, y, s=\"   %s\" % word, horizontalalignment='left')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rHzyAbP1_Cjv",
        "outputId": "32ae6cd0-a68f-4fc7-8a6e-9b0b3a4027da"
      },
      "outputs": [],
      "source": [
        "for q in ['steroids', 'drugs', 'lawsuit', 'puppy']:\n",
        "    display_attention_weights(keys='copyright law dog cat baseball football', query=q)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
