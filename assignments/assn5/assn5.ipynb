{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory Machine Learning: Assignment 5\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "Assignment 5 is due Thursday, December 5 at 11:59pm. Late work will not be accepted as per the course policies (see the Syllabus and Course policies on [Canvas](https://canvas.yale.edu).\n",
    "\n",
    "Directly sharing answers is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "You should start early so that you have time to get help if you're stuck. The drop-in office hours schedule can be found on [Canvas](https://canvas.yale.edu).  You can also post questions or start discussions on [Ed Discussion](https://edstem.org/us/courses/44592/discussion). The problems are broken up into steps that should help you to make steady progress.\n",
    "\n",
    "**Submission:**\n",
    "\n",
    "Submit your assignment as a .pdf on Gradescope, and as a .ipynb on Canvas. You can access Gradescope through Canvas on the left-side of the class home page. The problems in each homework assignment are numbered. Note: When submitting on Gradescope, please select the correct pages of your pdf that correspond to each problem. This will allow graders to find your complete solution to each problem.\n",
    "\n",
    "To produce the .pdf, please do the following in order to preserve the cell structure of the notebook:  \n",
    "1.  Go to \"File\" at the top-left of your Jupyter Notebook\n",
    "2.  Under \"Download as\", select \"HTML (.html)\"\n",
    "3.  After the .html has downloaded, open it and then select \"File\" and \"Print\" (note you will not actually be printing)\n",
    "4.  From the print window, select the option to save as a .pdf\n",
    "\n",
    "**Topics**\n",
    "1. Bayesian inference (20 points)\n",
    "2. Topic models (30 points)\n",
    "3. Neural networks OR Reinforcement learning (25 points)\n",
    "\n",
    "The first problem tests some of the basics of Bayesian inference and topic models. The second problem has you building topic models to improve pricing of houses on Zillow. The third problem gives you experience with neural networks in tensorflow. The fourth problem is a reinforcement learning task similar to the Taxi problem, but with a random environment.\n",
    "\n",
    "*IMPORTANT NOTE: For this assignment you need to do only question 3 OR question 4. We will grade only one of them even if you do both.*\n",
    "\n",
    "The assignment looks longer than it really is. We step you through most of the code that you need. But it's still on the long side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Problem 1:  [ABCD](https://en.wikipedia.org/wiki/ABCD_2): Bayesian Inference in A/B Testing\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this problem, we'll explore how Bayesian inference can be applied to A/B testing, a commonly used method in industry to evaluate the effectiveness of changes to a system. If A/B testing is new to you, don't worry! We'll introduce the concept step-by-step.\n",
    "\n",
    "Imagine you're working as a data scientist for an online company. You're tasked with analyzing the results of a new website design (variant B) compared to the current design (variant A) to see which one achieves a higher conversion rate (e.g., percentage of visitors who make a purchase). This scenario is perfect for Bayesian inference, as it allows us to calculate the probability that one version is better than the other, rather than just accepting or rejecting a hypothesis.\n",
    "\n",
    "### A/B Testing Overview\n",
    "\n",
    "A/B testing is a method where two versions (A and B) of a variable are compared by exposing half of the audience to each variant. We collect conversion data on each variant:\n",
    "- **Variant A**: the current design\n",
    "- **Variant B**: the new design\n",
    "\n",
    "For each variant, we record whether each visitor converted (made a purchase) or not. The goal is to determine whether variant B performs better than variant A by estimating the conversion rate of each.\n",
    "\n",
    "### Exercise Structure\n",
    "\n",
    "We'll proceed through several sections:\n",
    "\n",
    "1. **Understanding the dataset** - Loading and exploring A/B test data.\n",
    "2. **Setting priors** - Defining prior beliefs for conversion rates.\n",
    "3. **Calculating the posterior** - Using Bayesian inference to update our beliefs.\n",
    "4. **Decision making** - Estimating credible intervals and comparing variants.\n",
    "5. **Choosing an informed prior** - Integrating insights from similar experiments to select an appropriate prior.\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 1: Understanding the Dataset\n",
    "\n",
    "#### Conceptual Introduction\n",
    "\n",
    "You'll be working with a real-world dataset from an A/B test conducted by an e-commerce company. The dataset contains information about user interactions with two different versions of a web page: The control (variant A) and the treatment (variant B). Each row represents a user's interaction, indicating which variant they were exposed to and whether they converted (i.e., completed a desired action such as a purchase).\n",
    "\n",
    "#### 1.1 Load and Explore the Dataset\n",
    "\n",
    "We'll start by loading the dataset directly from an online source.\n",
    "\n",
    "Note: This dataset was published by Mostafa Elmehy in Kaggle 2020 [https://www.kaggle.com/datasets/mostafaelmehy/ab-data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/abel-keya/Project-Analyze-A-B-Test-Results/master/ab_data.csv'\n",
    "data = pd.read_csv(url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Explore the data by calculating the conversion rate for each variant and describe what you see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 2: Setting Priors\n",
    "#### 2.1 What is a prior in Bayesian inference, and why is it useful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2900a95",
   "metadata": {},
   "source": [
    "[Your markdown answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2 Implementing Priors\n",
    "We'll assume a Beta distribution as a prior, which is commonly used for binomial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import beta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "\n",
    "# Prior belief parameters\n",
    "alpha_prior, beta_prior = 0.5, 0.5\n",
    "\n",
    "# Plot prior\n",
    "x = np.linspace(0, 1, 100)\n",
    "plt.plot(x, beta.pdf(x, alpha_prior, beta_prior), label=\"Prior Distribution\")\n",
    "plt.xlabel(\"Conversion Rate\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Prior Belief on Conversion Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Experiment by changing the values of alpha_prior and beta_prior. How does the shape of the prior change?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 3: Calculating the Posterior\n",
    "#### Explain the concept of a posterior distribution in Bayesian inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.2 Updating with Observed Data\n",
    "#### Using the data, calculate the posterior distribution for each variant. Hint: Since each visitor either converts (success) or does not convert (failure), the data for each variant can be modeled as a binomial distribution. Plot the posterior distribution for each variant in the same figure. Show the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Compare the two posterior distributions. Which one has a higher mean, indicating a potentially higher conversion rate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Interpretation: If the posterior mean for the treatment group is higher than for the control, it suggests that the new variant may improve conversion rates. However, further analysis is needed to confirm this with statistical rigor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 4: Decision Making with Credible Intervals\n",
    "#### 4.1 Calculating Credible Intervals\n",
    "In Bayesian inference, a credible interval provides a range of values within which the true conversion rate is likely to fall, given our data. We’ll calculate the 95% credible interval for each variant’s conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "# Calculate 95% credible intervals\n",
    "ci_control = # your code here\n",
    "ci_treatment = # your code here\n",
    "\n",
    "# Set up x-axis values for plotting\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Define the x-axis limits based on credible intervals\n",
    "x_min = min(ci_control[0], ci_treatment[0]) - 0.02\n",
    "x_max = max(ci_control[1], ci_treatment[1]) + 0.02\n",
    "\n",
    "# Plot posterior distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "control_posterior = # your code here\n",
    "treatment_posterior = # your code here\n",
    "plt.plot(x, control_posterior, label=\"Control Posterior\", color=\"blue\")\n",
    "plt.plot(x, treatment_posterior, label=\"Treatment Posterior\", color=\"green\")\n",
    "\n",
    "# Shade the credible intervals for each variant\n",
    "plt.fill_between(x, 0, control_posterior,\n",
    "                 where=(x >= ci_control[0]) & (x <= ci_control[1]), color=\"blue\", alpha=0.2,\n",
    "                 label=\"Control 95% Credible Interval\")\n",
    "plt.fill_between(x, 0, treatment_posterior,\n",
    "                 where=(x >= ci_treatment[0]) & (x <= ci_treatment[1]), color=\"green\", alpha=0.2,\n",
    "                 label=\"Treatment 95% Credible Interval\")\n",
    "\n",
    "# Set x-axis limits\n",
    "plt.xlim(x_min, x_max)\n",
    "\n",
    "# Label the plot\n",
    "plt.xlabel(\"Conversion Rate\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(\"Posterior Distributions with 95% Credible Intervals\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the credible intervals for reference\n",
    "print(f\"95% Credible Interval for Control Group: [{ci_control[0]:.4f}, {ci_control[1]:.4f}]\")\n",
    "print(f\"95% Credible Interval for Treatment Group: [{ci_treatment[0]:.4f}, {ci_treatment[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " #### Interpret the credible intervals. Do they overlap significantly? If so, what does this indicate about the effectiveness of the treatment compared to the control?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Section 5: Experimenting with Different Priors\n",
    "#### Imagine that one of your coworkers recently completed an A/B test for a similar website, where the conversion rates for the control and treatment groups were both around 10%. This information could inform our initial beliefs about the conversion rates in our current A/B test. Given this prior knowledge from your coworker's results on a similar website, how would you choose an appropriate prior distribution for the current test? Describe the parameters you would select for the Beta distribution and explain your reasoning. Recalculate the posterior distributions for the control and treatment groups using this new prior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Compare these new posterior distributions with the original ones. How has the new prior influenced the posterior results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. [UP](https://en.wikipedia.org/wiki/Up_(2009_film)): Rising house prices  (30 points)\n",
    "\n",
    "![zillow](https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn5/zillow.png)\n",
    "\n",
    "### Overview of the problem\n",
    "\n",
    "Here we have a dataset of single family houses sold in Connecticut near the beginning of 2021, collected from [Zillow](https://www.zillow.com/homes/connecticut_rb/). You will build linear models of the price for which each house sold, based on its characteristics given in the real estate listing. Such characteristics include internal square footage, the year it was built, the bedroom count, the bathroom count, and the area of the lot. \n",
    "\n",
    "But there is also usually a lengthy description written by the real estate agent. Is there any additional information hidden in this description that would help improve the model of the price? This is the question we focus on in this problem.\n",
    "\n",
    "Answering such a question is difficult because the description is written in natural language with thousands of different words. Here we use topic models as a dimension reduction technique. Specifically, instead of using thousands of possible words, and how many times they show up in each house description, we reduce the words to the topic proportions $\\theta_d$ for each document, obtained by posterior inference. These proportions are combined with the other quantitative variables in a linear model with the logarithm of the house price as the response variable. \n",
    "\n",
    "*Important note:* At first glance, this problem looks really long. But this is deceiving. \n",
    "After reading in the data, we have you make some plots of the log-transformed variables. \n",
    "After that, you just need to run the code that leads up to training a 10-topic topic model, \n",
    "and fitting a linear model using the resulting topic proportions. After this, you are asked to compare the results to those obtained with a 3-topic model. To do this, you can simply copy the code used for the 10-topic model. After that, the crux of the problem is to analyze, understand, and describe the results.\n",
    "\n",
    "Acknowledgment: The data were scraped and the analysis was done by the amazing [Parker Holzer](https://parkerholzer.github.io/), as he began his search for a new house for his family after beginning a job as a data scientist. Thanks Parker!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.152805Z",
     "start_time": "2021-11-12T15:01:45.572491Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from collections import Counter\n",
    "import statsmodels.formula.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in and clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.594652Z",
     "start_time": "2021-11-12T15:01:47.154274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>BED</th>\n",
       "      <th>BATH</th>\n",
       "      <th>BUILT</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>LOTSIZE</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1629.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1889.0</td>\n",
       "      <td>Welcome home! Charming &amp; well kept, this 2 bed...</td>\n",
       "      <td>0.159986</td>\n",
       "      <td>224000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1278.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>This adorable cape has a lot to offer.  You st...</td>\n",
       "      <td>0.179981</td>\n",
       "      <td>225000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1264.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>This 1264 sqft Colonial with its 3 bedrooms an...</td>\n",
       "      <td>0.089991</td>\n",
       "      <td>224900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2054.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>The perfect oversized ranch awaits you at 7 No...</td>\n",
       "      <td>0.569994</td>\n",
       "      <td>370000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>Beautiful Colonial-3020 sqft. living space and...</td>\n",
       "      <td>0.939989</td>\n",
       "      <td>489999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>848.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>This home sets at the beginning of a Cul-de-Sa...</td>\n",
       "      <td>0.189990</td>\n",
       "      <td>429900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>New home to be built. Amazing unobstructed wat...</td>\n",
       "      <td>0.079981</td>\n",
       "      <td>800000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>6538.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Can you say water views galore? Wake up to the...</td>\n",
       "      <td>0.079981</td>\n",
       "      <td>2700000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>4480.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1890.0</td>\n",
       "      <td>NEW YEAR!  NEW FUTURE!    Escape NY to Connect...</td>\n",
       "      <td>0.849998</td>\n",
       "      <td>2550000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>One of the nicest new construction homes avail...</td>\n",
       "      <td>0.119995</td>\n",
       "      <td>1275000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1926 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AREA  BED  BATH   BUILT  \\\n",
       "0     1629.0  2.0   2.0  1889.0   \n",
       "1     1278.0  3.0   2.0  1900.0   \n",
       "2     1264.0  3.0   2.0  1988.0   \n",
       "3     2054.0  3.0   3.0  1960.0   \n",
       "4     4198.0  5.0   3.0  1972.0   \n",
       "...      ...  ...   ...     ...   \n",
       "1921   848.0  3.0   2.0  1948.0   \n",
       "1922  2400.0  4.0   4.0  2021.0   \n",
       "1923  6538.0  7.0   7.0  2002.0   \n",
       "1924  4480.0  5.0   5.0  1890.0   \n",
       "1925  3000.0  4.0   3.0  2020.0   \n",
       "\n",
       "                                            DESCRIPTION   LOTSIZE      PRICE  \n",
       "0     Welcome home! Charming & well kept, this 2 bed...  0.159986   224000.0  \n",
       "1     This adorable cape has a lot to offer.  You st...  0.179981   225000.0  \n",
       "2     This 1264 sqft Colonial with its 3 bedrooms an...  0.089991   224900.0  \n",
       "3     The perfect oversized ranch awaits you at 7 No...  0.569994   370000.0  \n",
       "4     Beautiful Colonial-3020 sqft. living space and...  0.939989   489999.0  \n",
       "...                                                 ...       ...        ...  \n",
       "1921  This home sets at the beginning of a Cul-de-Sa...  0.189990   429900.0  \n",
       "1922  New home to be built. Amazing unobstructed wat...  0.079981   800000.0  \n",
       "1923  Can you say water views galore? Wake up to the...  0.079981  2700000.0  \n",
       "1924  NEW YEAR!  NEW FUTURE!    Escape NY to Connect...  0.849998  2550000.0  \n",
       "1925  One of the nicest new construction homes avail...  0.119995  1275000.0  \n",
       "\n",
       "[1926 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ct_homes = pd.read_csv('https://raw.githubusercontent.com/YData123/sds265-fa23/main/assignments/assn5/ct_zillow.csv')\n",
    "ct_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data\n",
    "\n",
    "We add columns to `ct_homes` called `logAREA`, `logLOTSIZE`, and `logPRICE` that take the logarithms of the corresponding columns in the original data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.614588Z",
     "start_time": "2021-11-12T15:01:47.596230Z"
    }
   },
   "outputs": [],
   "source": [
    "ct_homes['logAREA'] = np.log(ct_homes['AREA'])\n",
    "ct_homes['logLOTSIZE'] = np.log(ct_homes['LOTSIZE'])\n",
    "ct_homes['logPRICE'] = np.log(ct_homes['PRICE'])\n",
    "ct_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Plot the data \n",
    "\n",
    "1. Show histograms of each of the log-transformed columns.\n",
    "\n",
    "1. Our regression models will use these transformed values. Why might it be preferable to use the logarithms rather than the original data? Explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your code and markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the descriptions as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.620301Z",
     "start_time": "2021-11-12T15:01:47.617177Z"
    }
   },
   "outputs": [],
   "source": [
    "example = 9\n",
    "ct_homes[\"DESCRIPTION\"][example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions\n",
    "\n",
    "The following two functions will be used to clean up the text a bit and separate into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.624584Z",
     "start_time": "2021-11-12T15:01:47.621530Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_description(desc):\n",
    "    if type(desc) == float:\n",
    "        desc = \"\"\n",
    "    words = [re.sub(r'[^a-z]', '', w) for w in desc.lower().split(' ')]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def reduce_to_vocabulary(desc, vocab):\n",
    "    return ' '.join([w for w in cleanup_description(desc).split(' ') if w in vocab])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.629948Z",
     "start_time": "2021-11-12T15:01:47.625889Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup_description(ct_homes['DESCRIPTION'][example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Next we build a vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.919063Z",
     "start_time": "2021-11-12T15:01:47.631311Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "for dsc in ct_homes['DESCRIPTION']:\n",
    "    vocab.update(cleanup_description(dsc).split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:47.923544Z",
     "start_time": "2021-11-12T15:01:47.920814Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove words that are either too common or too rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.081138Z",
     "start_time": "2021-11-12T15:01:47.925007Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = Counter(token for token in vocab.elements() if vocab[token] > 5)\n",
    "stop_words = [item[0] for item in vocab.most_common(50)]\n",
    "vocab = Counter(token for token in vocab.elements() if token not in stop_words)\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a mapping between unique words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.088577Z",
     "start_time": "2021-11-12T15:01:48.084325Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc = ct_homes['DESCRIPTION'][example]\n",
    "print('Original description:\\n---------------------')\n",
    "print(desc)\n",
    "\n",
    "print('\\nCleaned up text:\\n----------------')\n",
    "print(cleanup_description(desc))\n",
    "\n",
    "print('\\nReduced to vocabulary:\\n----------------------')\n",
    "print(reduce_to_vocabulary(desc, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a mapping between unique words and integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.097985Z",
     "start_time": "2021-11-12T15:01:48.091685Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = {idx: pair[0] for idx, pair in enumerate(vocab.items())}\n",
    "word2id = {pair[0]: idx for idx, pair in enumerate(vocab.items())}\n",
    "\n",
    "s = 'nyc'\n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for '%s': %d\" % (s,word2id[s]))\n",
    "print(\"Word for identifier %d: %s\" % (word2id[s], id2word[word2id[s]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map to word id format\n",
    "\n",
    "Now, use the format required to build a language model, mapping each word to its id, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.656600Z",
     "start_time": "2021-11-12T15:01:48.101461Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for dsc in ct_homes['DESCRIPTION']:\n",
    "    clean = reduce_to_vocabulary(cleanup_description(dsc), vocab)\n",
    "    toks = clean.split(' ')\n",
    "    tokens.append(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:48.724420Z",
     "start_time": "2021-11-12T15:01:48.658177Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for toks in tokens:\n",
    "    tkn_count = Counter(toks)\n",
    "    corpus.append([(word2id[item[0]], item[1]) for item in tkn_count.items()])\n",
    "    \n",
    "dsc = ct_homes['DESCRIPTION'][example]\n",
    "clean = reduce_to_vocabulary(cleanup_description(dsc), vocab)\n",
    "toks = clean.split(' ')\n",
    "print(\"Abstract, tokenized:\\n\", toks, \"\\n\")\n",
    "print(\"Abstract, in corpus format:\\n\", corpus[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Topic Model with 10 topics\n",
    "\n",
    "Note: Don't worry about the various settings used in the call to `LdaModel`. If you want to read up on these, just check out the documentation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.387268Z",
     "start_time": "2021-11-12T15:01:48.725885Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tm = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                     id2word=id2word,\n",
    "                                     num_topics=10, \n",
    "                                     random_state=100,\n",
    "                                     chunksize=100,\n",
    "                                     passes=10,\n",
    "                                     alpha='auto',\n",
    "                                     per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.405918Z",
     "start_time": "2021-11-12T15:01:53.388536Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "num_words = 15\n",
    "top_words = pd.DataFrame({'word rank': np.arange(1,num_words+1)})\n",
    "for k in np.arange(num_topics): \n",
    "    topic = tm.get_topic_terms(k, num_words)\n",
    "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words['topic %d' % k] = words\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.415623Z",
     "start_time": "2021-11-12T15:01:53.407114Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_dist = tm.get_document_topics(corpus[example])\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topic_dist_table = pd.DataFrame()\n",
    "topic_dist_table['Topic'] = topics\n",
    "topic_dist_table['Probabilities'] = probabilities\n",
    "topic_dist_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:53.704131Z",
     "start_time": "2021-11-12T15:01:53.417119Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(11,4)\n",
    "plt.bar(topic_dist_table['Topic'], topic_dist_table['Probabilities'], align='center', alpha=1, color='salmon')\n",
    "plt.xlabel('topic')\n",
    "plt.ylabel('probability')\n",
    "plt.title('Per Topic Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include the topic proportions $\\theta_d$ for each house \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.326782Z",
     "start_time": "2021-11-12T15:01:53.705671Z"
    }
   },
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "theta = pd.DataFrame({\"Theta0\": np.zeros(ct_homes.shape[0])})\n",
    "for t in np.arange(1,num_topics):\n",
    "    theta[\"Theta\"+str(t)] = np.zeros(ct_homes.shape[0])\n",
    "    \n",
    "for i in np.arange(ct_homes.shape[0]):\n",
    "    for t in tm.get_document_topics(corpus[i]):\n",
    "        theta.loc[i,\"Theta\"+str(t[0])] = t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.352081Z",
     "start_time": "2021-11-12T15:01:56.328006Z"
    }
   },
   "outputs": [],
   "source": [
    "ct_topics = ct_homes.join(theta)\n",
    "ct_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a linear model with the topic proportions included\n",
    "\n",
    "We now fit a linear model with the topic proportions included. Note that \n",
    "the proportions satisfy $\\theta_0+\\theta_1+\\cdots + \\theta_9 = 1$. Therefore, we remove one of them, since it is redundant. If we don't do this the linear model will be harder to interpret!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.398437Z",
     "start_time": "2021-11-12T15:01:56.353552Z"
    }
   },
   "outputs": [],
   "source": [
    "model = sm.ols(\"logPRICE ~ logAREA + logLOTSIZE + BED + BATH + BUILT + Theta0 + \" +\n",
    "               \"Theta1 + Theta2 + Theta3 + Theta4 + Theta5 + Theta6 + Theta7 + Theta8\", data=ct_topics).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.604079Z",
     "start_time": "2021-11-12T15:01:56.399978Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(model.resid, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model without the topics included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.627406Z",
     "start_time": "2021-11-12T15:01:56.605673Z"
    }
   },
   "outputs": [],
   "source": [
    "model_without_topics = sm.ols(\"logPRICE ~ logAREA + logLOTSIZE + BED + BATH + BUILT\", data=ct_topics).fit()\n",
    "model_without_topics.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Plot the residuals\n",
    "\n",
    "On a single plot, show a histogram of the residuals of the model without the topics, \n",
    "and the residuals of the model with the topics. Give a legend that shows which is which.\n",
    "Comment on the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.630729Z",
     "start_time": "2021-11-12T15:01:56.628696Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Quantify the improvement: R-squared\n",
    "\n",
    "How do the two models compare in terms of R-squared? What do these numbers mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Quantify the improvement: MSE decrease\n",
    "\n",
    "What is the percent decrease in the mean-squared-error of the model with the topics\n",
    "compared to the model that ignores the descriptions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.634159Z",
     "start_time": "2021-11-12T15:01:56.632262Z"
    }
   },
   "outputs": [],
   "source": [
    "# [your code and markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Quantify the improvement: LOOCV\n",
    "\n",
    "What is the percent decrease in the leave-one-out-cross-validation (LOOCV) error?\n",
    "Recall from class that the following formula can be used to calculate this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YData123/sds265-fa21/main/assignments/assn6/loocv.png\" width=\"410\" align=\"center\">\n",
    "\n",
    "<br>\n",
    "\n",
    "The following line of code computes this for one of the models:\n",
    "\n",
    "`np.mean((model.resid/(1 - model.get_influence().hat_matrix_diag))**2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.637660Z",
     "start_time": "2021-11-12T15:01:56.635497Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Repeat for three topics \n",
    "\n",
    "Now, repeat the above steps for a topic model that is trained using only three (3) topics. Specifically:\n",
    "\n",
    "1. Train a model with three topics\n",
    "1. Display the top words in each of the three topics\n",
    "1. Augment the `ct_homes` data with the resulting topic proportions $\\theta$\n",
    "1. Fit a linear model *using only the first two of the three* proportions\n",
    "1. Plot a histogram of the residuals of the three linear models together\n",
    "1. Comment on the improvement over the baseline in terms of R-squared, MSE, and LOOCV compared with the previous two models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-12T15:01:56.641409Z",
     "start_time": "2021-11-12T15:01:56.639322Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code and markdown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Interpretation\n",
    "\n",
    "Now, interpret the model. Use the coefficients of the linear model to \n",
    "help interpret the meaning of the topics. Comment on what this says \n",
    "about the effectiveness of the topic model for predicting the sale price \n",
    "of the house. Does it make intuitive sense? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your markdown here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: [Inside Out](https://en.wikipedia.org/wiki/Inside_Out_(2015_film)): Deep neural networks with tensorflow (25 points) \n",
    "\n",
    "*If you choose this question skip problem 4!*\n",
    "\n",
    "In class, we discussed a \"bare bones\" implementation of a 2-layer neural network for classification, using rectified linear units as activation functions. \n",
    "This implementation only uses the `numpy` package. In practice, we don't need to implement each component of a neural network from scratch. There exists software libraries like Tensorflow and PyTorch which make the process of building and training neural networks much faster. At their core, such libraries implement automatic differentiation so that the use only needs to define the \"forward pass\" of their model, and, as long as they do so within the framework, the gradients can be computed automatically. In addition, these libraries also have many built-in definitions of common neural network components. [Tensorflow](https://www.tensorflow.org/) is an open-source package for building and training neural networks. We will use it in this problem to explore applying neural networks to some synthetic classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a 2-layer neural network for classification takes the following form,\n",
    "\\begin{aligned}\n",
    "    h_{1} &=\\operatorname{ReLU}\\left(W_{1} X+b_{1}\\right) \\\\\n",
    "    p &=\\operatorname{Softmax}\\left(W_{2} h_{1}+b_{2}\\right).\n",
    "\\end{aligned}\n",
    "where $X$ is the input. The number of layers is referred to as the \"depth\" of a neural network. We can construct neural networks with arbitrary depth via the following architecture,\n",
    "\\begin{aligned}\n",
    "    h_{1} &=\\operatorname{ReLU}\\left(W_{1} X+b_{1}\\right) \\\\\n",
    "    h_{2} &=\\operatorname{ReLU}\\left(W_{2} h_1 + b_{2}\\right) \\\\\n",
    "    h_{3} &=\\operatorname{ReLU}\\left(W_{3} h_2 + b_{3}\\right) \\\\\n",
    "    &\\vdots\\\\\n",
    "    p &=\\operatorname{Softmax}\\left(W_{\\ell} h_{\\ell-1}+b_{\\ell}\\right).\n",
    "\\end{aligned}\n",
    "This is an $\\ell$-layer neural network. The $i$-th layer takes the output of the previous layer, $h_{i-1}$ as input, and produces $h_i$. The parameters of the $i$-th layer are the weights matrix $W_i$ and the bias vector $b_i$. The nonlinear activation function is $\\operatorname{ReLU}$ at each hidden layer, and $\\operatorname{Softmax}$ at the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we will learn how to train deep neural networks with Tensorflow and apply them to synthetic classification tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T20:54:09.927009Z",
     "start_time": "2021-11-29T20:54:09.572756Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# plt.rcParams['axes.facecolor'] = 'lightgray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate and plot a spiral dataset that contains $K=3$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T20:54:11.516809Z",
     "start_time": "2021-11-29T20:54:11.296972Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_spirals(N=100, K=3, noise=0.3):\n",
    "    D = 2  # dimensionality\n",
    "    X = np.zeros((N*K, D))\n",
    "    y = np.zeros(N*K, dtype='uint8')\n",
    "    for j in range(K):\n",
    "        ix = range(N*j, N*(j+1))\n",
    "        r = np.linspace(0.0, 1, N)  # radius\n",
    "        t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*noise  \n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        y[ix] = j\n",
    "    return X, y\n",
    "\n",
    "def plot_data(X, y):\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(np.min(X[:,0])-.1, np.max(X[:,0])+.1)\n",
    "    plt.ylim(np.min(X[:,1])-.1, np.max(X[:,1])+.1)\n",
    "\n",
    "\n",
    "def plot_classifier(X, y, model):\n",
    "    h = 0.015\n",
    "    x_min, x_max = X[:, 0].min() - .2, X[:, 0].max() + .2\n",
    "    y_min, y_max = X[:, 1].min() - .2, X[:, 1].max() + .2\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    # Z = np.dot(np.maximum(\n",
    "    #     0, np.dot(np.c_[xx.ravel(), yy.ravel()], W1) + b1), W2) + b2\n",
    "    inputs = np.array([xx.ravel(), yy.ravel()]).T\n",
    "    Z = model(inputs)\n",
    "    Z = np.argmax(Z, axis=1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig = plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    \n",
    "def plot_history(history, val=False, figsize=(8,3)):\n",
    "    '''plot given attributes from training history'''\n",
    "    plot_attrs = ('loss', 'accuracy')\n",
    "    fig, axs = plt.subplots(ncols=len(plot_attrs), figsize=figsize)\n",
    "\n",
    "    if not all(plot_attr in history.history for plot_attr in plot_attrs):\n",
    "        raise ValueError('not all `plot_attrs` are in the history object')\n",
    "\n",
    "    for plot_attr, ax in zip(plot_attrs, axs):\n",
    "        ax.plot(history.history[plot_attr], label=plot_attr)\n",
    "        if val:\n",
    "            ax.plot(history.history[f'val_{plot_attr}'], label=f'val_{plot_attr}')\n",
    "        ax.set_ylabel(plot_attr)\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def train_neurnet(neurnet, X, y, X_val, y_val, n_epochs=1_000, batch_size=128):\n",
    "    neurnet.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    history = neurnet.fit(\n",
    "        X, y, validation_data=(X_val, y_val), epochs=n_epochs, batch_size=batch_size, verbose=0,\n",
    "        callbacks=[TqdmCallback(data_size = len(y), batch_size=batch_size, verbose=0)])\n",
    "    plot_history(history, val=True);\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T20:54:11.516809Z",
     "start_time": "2021-11-29T20:54:11.296972Z"
    }
   },
   "outputs": [],
   "source": [
    "X_spiral, y_spiral = generate_spirals(N=300, K=3)\n",
    "X_spiral_val, y_spiral_val = generate_spirals(N=50, K=3)\n",
    "X_spiral_test, y_spiral_test = generate_spirals(N=300, K=3)\n",
    "plot_data(X_spiral, y_spiral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train a 2-layer neural network to classify this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# demo of neurnet\n",
    "neurnet = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "neurnet.build(input_shape=(None, 2))\n",
    "neurnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = train_neurnet(neurnet, X_spiral, y_spiral, X_spiral_val, y_spiral_val, n_epochs=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_classifier(X_spiral, y_spiral, neurnet)\n",
    "plot_classifier(X_spiral_test, y_spiral_test, neurnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.1: Experiment with different architectures and hyperparameters settings\n",
    "\n",
    "[Tensorflow playground](https://playground.tensorflow.org/) is a tool created by Tensorflow to demo how neural networks work, allowing users to tinker with different architectures on a few toy classification tasks (in fact, the same ones we are playing with in this problem). Under the hood, Tensorflow playground is running code like the one we are using in this notebook. Feel free to explore Tensorflow playground first before we continuing with this problem.\n",
    "\n",
    "Using the above code, experiment with different architectures and hyperparameters of the optimization. In particular, try the following two variations.\n",
    "1. Wider network. Increase the number of hidden units in the 2-layer network above. (hint: use the `units` argument in `layers.Dense`)\n",
    "2. Deeper network. Increase the depth of the network by adding an additional hidden layer. (hint: add an additional `layers.Dense`)\n",
    "\n",
    "For each variant, define your network using tensorflow, train it on the spiral data, visualize the decision boundaries, and include a markdown cell briefly describing what you observe. Feel free to add some additional variants as well. For example, you could train for longer, change the activation functions, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wider network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deeper network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3.2: Apply to three other toy datasets\n",
    "\n",
    "Now, choose an architecture you like and apply your neural network to three more synthetic classification tasks generated by sklearn. For each of the following, define your neural network, train it on the dataset, and visualize the decision boundary. Similar to the above, feel free to experiment with different architectures to improve the performance of your neural network on each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Circles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:29:50.361365Z",
     "start_time": "2021-11-29T21:29:50.237354Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# generate circles\n",
    "X_circles, y_circles = datasets.make_circles(300, noise=0.06, random_state=265)\n",
    "X_circles_val, y_circles_val = datasets.make_circles(100, noise=0.06, random_state=565)\n",
    "X_circles_test, y_circles_test = datasets.make_circles(300, noise=0.06, random_state=565)\n",
    "\n",
    "plot_data(X_circles, y_circles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first toy dataset forms two concentered circles with different radius, and we add a little bit of noise so that two circles start mixing together. Train a neural network on this taks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Blobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:32:27.007059Z",
     "start_time": "2021-11-29T21:32:26.903357Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate blobs\n",
    "centers = np.array([[-.5,.0],[0,.5], [.5,.1], [-.2, -.7], [.2, -.3]])\n",
    "X_blobs, y_blobs = datasets.make_blobs(300, centers=centers, cluster_std=.15, center_box=(-1.0, 1.0), random_state=265)\n",
    "X_blobs_val, y_blobs_val = datasets.make_blobs(100, centers=centers, cluster_std=.15, center_box=(-1.0, 1.0), random_state=565)\n",
    "X_blobs_test, y_blobs_test = datasets.make_blobs(300, centers=centers, cluster_std=.15, center_box=(-1.0, 1.0), random_state=565)\n",
    "\n",
    "plot_data(X_blobs, y_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset forms three clusters centered at different places. Repeat the procedure that you have walked through for the first dataset. Note that this time the number of classes is 5 not 3, so you'll have to adjust the number of units in the final unit accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T23:27:18.764555Z",
     "start_time": "2021-11-28T23:27:18.762744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Moons\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T21:39:22.155382Z",
     "start_time": "2021-11-29T21:39:22.026240Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate moons\n",
    "X_moons, y_moons = datasets.make_moons(300, noise=0.2, random_state=265)\n",
    "X_moons_val, y_moons_val = datasets.make_moons(100, noise=0.2, random_state=565)\n",
    "X_moons_test, y_moons_test = datasets.make_moons(300, noise=0.2, random_state=565)\n",
    "\n",
    "plot_data(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third dataset is composed of two moon-shaped clusters. Repeat the procedures above on this dataset. Note that the number of classes is 2 now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Lake_Baikal_in_winter.jpg\" width=300 style=\"padding: 10px; float: right;\">\n",
    "\n",
    "\n",
    "## Problem 4: [Frozen](https://en.wikipedia.org/wiki/Frozen_(2013_film)): Navigating a random environment (25 points) \n",
    "\n",
    "*If you choose this question skip problem 3!*\n",
    "\n",
    "In class we introduce the Q-learning algorithm using the Taxi problem from the OpenAI `gym` package. In this problem we will explore another toy reinforcement learning problem, called  \"Frozen Lake\". In this problem you need to walk over a grid that represents a partially frozen lake, being careful not to fall through holes in the ice. (The author of this problem fell through the ice in a frozen lake when he was a kid&mdash;at night! It was quite an experience...) The environment is a simple 4x4 grid, and the goal is to walk from one corner to the other without falling through.\n",
    "\n",
    "Unlike the Taxi problem, but more like Tic-Tac-Toe as discussed in class, there are no intermediate rewards. Rather the reward is 1 if you succeed, and 0 otherwise. `Frozen Lake` has two versions. In the first version, the ice is (unrealistically) not slippery. Here the state transitions are deterministic: If you move right, you go right. In the second version, the state transitions are probabilistic: If you try to move right, you may go left, down, or up. Naturally, the slippery version is more challenging. \n",
    "\n",
    "Your task in this problem will be to complete the implementation of the Q-learning algorithm for this problem, display the value function, and then evaluate the solution. You'll do this for both the deterministic and random (slippery) versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the necessary packages. You'll probably need to install `gym`, and can use `!pip install gym`. If you have difficulties let us know and we'll try to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the \"ascii art\" rendition of the starting state. You start in the upper left corner, marked `S`, and the goal is to get to the lower right corner, marked `G`. The ice has four holes, marked `H`; if you step here you fall through the ice and the episode is done. But as you learn, you do not know where the holes are. We'll first use the deterministic version, by specifying `is_slippery=False`.\n",
    "\n",
    "This code was written and tested with `gym` version 0.26.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some simple helper functions, same as we used for the Taxi demo. Don't change the cell below, just run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, stat, action, reward):\n",
    "    return {'frame': env.render(),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward}\n",
    "\n",
    "\n",
    "def print_frames(frames, delay=.1):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(delay)\n",
    "        \n",
    "def display_value_function(q_table):\n",
    "    v = np.max(q_table, axis=1)\n",
    "    plt.imshow(v.reshape(4,4))\n",
    "    plt.axis('off')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(np.round(v.reshape(4,4), 3))\n",
    "\n",
    "def evaluate_Q_function(env, q_table, epsilon=.001, episodes=1000):\n",
    "    total_steps, total_successes = 0, 0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state, *_ = env.reset()\n",
    "        steps, reward = 0, 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "               action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "    \n",
    "            next_state, reward, done, *_ = env.step(action)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "        total_successes += reward\n",
    "        total_steps += steps\n",
    "    \n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average steps per episode: {total_steps / episodes}\")\n",
    "    print(f\"Chance of success: {total_successes / episodes}\")\n",
    "    \n",
    "def sample_episode(env, q_table, epsilon=.001):\n",
    "    state, *_ = env.reset()\n",
    "    steps, reward = 0, 0\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        state, reward, done, *_ = env.step(action)\n",
    "        frames.append(render(env, state, action, reward))\n",
    "        steps += 1\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we simply walk around randomly, calling the `sample()` function to choose an action. At the end of the episode, the steps are displayed. If you're lucky, you'll make it to the goal; more likely is that you fall through the ice. Try it out a few times to make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "steps = 0\n",
    "reward = 0\n",
    "frames = [] \n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample() # choose a random action\n",
    "    state, reward, done, *_ = env.step(action)\n",
    "    frames.append(render(env, state, action, reward))\n",
    "    steps += 1\n",
    "    \n",
    "\n",
    "print_frames(frames, delay=.5)\n",
    "print(f\"\\nSteps taken: {steps}, success: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.1: Complete the Implementation of Q-learning\n",
    "\n",
    "In the cell below we have started you off with a partial implementation of Q-learning. Your job is to finish the implementation. You'll then use your implementation to learn the Q-function for both the deterministic and random versions of the environment. \n",
    "\n",
    "The function declaration looks like this:\n",
    "\n",
    "```python\n",
    "def Q_learning(env, alpha=.1, gamma=.7, epsilon=.1, episodes=10000):\n",
    "```\n",
    "\n",
    "with the following arguments:\n",
    "\n",
    "* `env` is the environment. We pass this in because we're going to have slippery and non-slippery versions.\n",
    "* `alpha` is the step size\n",
    "* `gamma` is the discount for future rewards\n",
    "* `epsilon` is the probability of exploring\n",
    "* `episodes` is the number of training episodes to use. \n",
    "\n",
    "Some hints:\n",
    "\n",
    "* The Q-table is intialized to have all values 1/2. When you transition to a state and the value of `done` is `True`, there are two possibilities: (1) You fell through a hole in the ice or (2) you reached the goal (congratulations!)\n",
    "\n",
    "* The value of the Q-function for these states (with all actions) should be zero; no future reward is possible.  You don't know where the holes in the ice are (even though you could \"cheat\" and read them off the rendering of the environment.\n",
    "\n",
    "* If you reached the goal, which is state 15, the value of the reward when you transition to this state will be 1.\n",
    "\n",
    "* All values of the Q-function should be less than or equal to 1---the value can be interpreted as the probability of reaching the goal starting from the state/action pair.\n",
    "\n",
    "* You should only need 3-5 lines of code to complete the implementation! If you find yourself using more, you may want to rethink your approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, alpha=.1, gamma=.7, epsilon=.1, episodes=10000):\n",
    "    q_table = 0.5*np.ones([env.observation_space.n, env.action_space.n])\n",
    "    for _ in tqdm(np.arange(episodes)):\n",
    "        state, *_ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            explore = False\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                explore = True\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "            next_state, reward, done, *_ = env.step(action) \n",
    "            ### Finish the implementation below. Only 3-5 lines of code needed.\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.2\n",
    "\n",
    "Now use the function `Q_learning` that you just wrote, and explore different settings \n",
    "of the parameters. The following cells carry out the following steps:\n",
    "\n",
    "1. Create the environment\n",
    "1. Run Q-learning. This is where you can experiment with different parameters\n",
    "1. Display the value function `v(s) = np.max(q_table[s])`\n",
    "1. Describe the value function, and how the numerical values make sense\n",
    "1. Run `evaluate_Q_function` to get statistics on how well it works\n",
    "1. Comment on the evaluation statistics\n",
    "1. Print out a sample episode\n",
    "\n",
    "The only line you need to change is the call to `Q_learning`, which is where you \n",
    "select the parameters. *Do not change the other cells*. You will be graded according \n",
    "to your implementation; these are checks to make sure it is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only change the following line, to set parameters\n",
    "q_table = Q_learning(env, alpha=.1, gamma=.5, epsilon=.5, episodes=100)\n",
    "display_value_function(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the value function above. Do the numerical values in different states make sense? Why or why not? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "evaluate_Q_function(env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the evaluate statistics make sense? How do they compare to the random environment below? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "frames = sample_episode(env, q_table)\n",
    "print_frames(frames, delay=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4.3\n",
    "\n",
    "Now use the function `Q_learning` on the random environment, \n",
    "where `is_slippery=True`. You may want to play around with this a bit \n",
    "to make sure you understand how it differs from the case where `is_slippery=False`.\n",
    "The difference is that there is randomness in the state transitions for each \n",
    "action. If you try to go down, you may go right, for example.\n",
    "\n",
    "As above, you run the following steps:\n",
    "\n",
    "1. Create the environment\n",
    "1. Run Q-learning. This is where you can experiment with different parameters\n",
    "1. Display the value function `v(s) = np.max(q_table[s])`\n",
    "1. Describe the value function, and how the numerical values make sense\n",
    "1. Run `evaluate_Q_function` to get statistics on how well it works\n",
    "1. Comment on the evaluation statistics\n",
    "1. Print out a sample episode\n",
    "\n",
    "The only line you need to change is the call to `Q_learning`, which is where you \n",
    "select the parameters. *Do not change the other cells*. You will be graded according \n",
    "to your implementation; these are checks to make sure it is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change\n",
    "random_env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode='ansi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only change the following line, to set parameters\n",
    "q_table = Q_learning(random_env, alpha=.1, gamma=.5, epsilon=.5, episodes=100)\n",
    "display_value_function(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the value function above. Do the numerical values in different states make sense? Why or why not? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "evaluate_Q_function(random_env, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the output statistics make sense? How do they compare to the deterministic environment?Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[your markdown here]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run this cell\n",
    "frames = sample_episode(random_env, q_table)\n",
    "print_frames(frames, delay=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[Add any discussion of your implementation and findings that you wish to share. Nothing specific is required.]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus question\n",
    "\n",
    "What is the common theme of all of the problem names?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
